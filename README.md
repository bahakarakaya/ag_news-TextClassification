> 📄 **For the full project evaluation and analysis report: [metric_report.md](./metric_report.md)**

# 📄 LLM Text Classification Report

This project aims to classify news articles on the AG News dataset using different prompt strategies and compare these strategies in terms of accuracy and efficiency.

## 🎯 Objective

- To compare the classification performance of different prompt structures (zero-shot, few-shot, CoT, etc.)
- To measure the accuracy and token efficiency in the outputs generated by the LLM
- To provide recommendations on the performance/cost balance according to the strategies

## 🧠 LLM Used

- **Model:** `gemma-3-12b-it`
- **API:** `google-generativeai` Python SDK
- **Outputs:** Directly generated by the LLM with no manual intervention.

## 🧪 Test Details

- **Dataset:** AG News (via HuggingFace)
- **Test sample:** 100 random news articles (seed=42)
- **Method:** Accuracy, precision, recall, and F1 were calculated using macro-average

## 📊 Sample Outputs

All test results, confusion matrix images, and performance metrics are located in the `stats/` folder.

| Strategy              | Accuracy | F1 Score | Total Tokens | Accuracy/Token |
|-----------------------|----------|----------|--------------|----------------|
| Role + Few-Shot + CoT | 93%      | 0.932    | 486.56       | 0.00191        |
| Zero-Shot + Role      | 89%      | 0.889    | 114.98       | 0.00774        |
| ...                   | ...      | ...      | ...          | ...            |


## 🛠 Setup

```bash
# Create environment
python -m venv venv
source venv/bin/activate # Windows: venv\Scripts\activate
```

# Install requirements
```bash
pip install -r requirements.txt
```

## 🔐 Environment Variable
Define your Gemini API key as a .env file:
```bash
# .env
GEMINI_API_KEY=your_api_key_here
```

## 🚀 Run
```bash
python main.py
```

The code runs inference on 100 news articles, gets the predictions, calculates the token counts, and saves the metrics to the stats/ folder.


### 📌 Notes
* The metric_report.md file contains all prompts, tables, evaluations, and final recommendations.
* Prompt examples and LLM outputs are also included in the report.